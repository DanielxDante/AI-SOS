{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The filename, directory name, or volume label syntax is incorrect.\n"
     ]
    }
   ],
   "source": [
    "%pip install mediapipe opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The filename, directory name, or volume label syntax is incorrect.\n"
     ]
    }
   ],
   "source": [
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp # mediapipe by Google as our ML solution for live and streaming media data\n",
    "import cv2 # openCV for webcam\n",
    "import numpy as np # math mod|ule for mediapipe\n",
    "import uuid # uniform unique identifier creates a unique string for images\n",
    "import os \n",
    "from matplotlib import pyplot as plt\n",
    "import tkinter\n",
    "from tkinter import messagebox # alert once a gesture is detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils # able to create red dots, which are landmarks\n",
    "mp_hands = mp.solutions.hands # a hands model with default landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[WinError 183] Cannot create a file when that file already exists: 'Output Images'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13676\\2311067413.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# save webcam frames for reference\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Output Images\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m: [WinError 183] Cannot create a file when that file already exists: 'Output Images'"
     ]
    }
   ],
   "source": [
    "# create folder for saved webcam frames\n",
    "os.mkdir(\"Output Images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Build Hand Detection using MediaPipe\n",
    "\n",
    "Let's explore the capabilities of MediaPipe by Google!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://mediapipe.dev/images/mobile/hand_landmarks.png>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index refers to the hand result; dependent on number of hands on screen\n",
    "# hand refers to the actual hand landmarks\n",
    "# results refers to our hand axis data\n",
    "def get_label(index, hand, results):\n",
    "    output = None\n",
    "    for idx, classification in enumerate(results.multi_handedness): \n",
    "        if idx == index: # checking for correct hand\n",
    "            # Extract data\n",
    "            label = classification.classification[0].label\n",
    "            score = classification.classification[0].score\n",
    "            text = \"{} {}\".format(label, round(score, 2))\n",
    "\n",
    "            # Extract coordinates \n",
    "            # [640, 480] refers to the dimensions of your webcam, change it accordingly\n",
    "            coords = tuple(\n",
    "                np.multiply(np.array((hand.landmark[mp_hands.HandLandmark.WRIST].x, hand.landmark[mp_hands.HandLandmark.WRIST].y)),\n",
    "                [640, 480]).astype(int))\n",
    "\n",
    "            output = text, coords\n",
    "        \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 angles shown - 5 for each finger, 1 for wrist \n",
    "joint_list = [[4, 3, 2], [8, 7, 6], [12, 11, 10], [16, 15, 14], [20, 19, 18], [1, 0, 5]] # joints of fingers we will calculate angle from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_finger_angles(image, results, joint_list):\n",
    "    angles = []\n",
    "    # Loop through hands\n",
    "    for hand in results.multi_hand_landmarks:\n",
    "        #Loop through joint sets \n",
    "        for joint in joint_list:\n",
    "            a = np.array([hand.landmark[joint[0]].x, hand.landmark[joint[0]].y]) # First coord\n",
    "            b = np.array([hand.landmark[joint[1]].x, hand.landmark[joint[1]].y]) # Second coord\n",
    "            c = np.array([hand.landmark[joint[2]].x, hand.landmark[joint[2]].y]) # Third coord\n",
    "            \n",
    "            # formula to calculate angle\n",
    "            radians = np.arctan2(c[1] - b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])\n",
    "            angle = np.abs(radians*180.0/np.pi)\n",
    "            \n",
    "            if angle > 180.0: # reference to upright finger\n",
    "                angle = 360 - angle\n",
    "            \n",
    "            angles.append(angle)\n",
    "            \n",
    "            # print to image\n",
    "            cv2.putText(image, str(round(angle, 2)), tuple(np.multiply(b, [640, 480]).astype(int)),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0) # the input number 0 depends on your machine, it could be 1 or 2 as well\n",
    "\n",
    "# Detection Confidence: Threshold for initial detection to be successful\n",
    "# Tracking Confidence: Threshold for tracking after initial detection\n",
    "with mp_hands.Hands(min_detection_confidence = 0.5, min_tracking_confidence = 0.5, max_num_hands = 4) as hands:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read() # returns a return value and frame which is the image\n",
    "\n",
    "        # Get hand axis data from each frame; default 2 hands detectable\n",
    "        # In case your webcam is so old that it uses the color format BGR instead of RGB, uncomment the appropriate lines\n",
    "        image = cv2.flip(frame, 1) # flip on horizontal\n",
    "        # image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # webcam frame colour format is in BGR, but mediapipe takes in RGB\n",
    "        image.flags.writeable = False # lock the frame for us to process its data\n",
    "        results = hands.process(image) # get axis data\n",
    "        image.flags.writeable = True # unlock the frame for us to draw landmarks and connections\n",
    "        # image = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR) # convert mediapipe image back to BGR for webcam\n",
    "        \n",
    "        cv2.putText(image, \"Status: Safe\", (0, 25), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        if results.multi_hand_landmarks: # if we detect a hand, we draw the landmarks and the connections\n",
    "            for num, hand in enumerate(results.multi_hand_landmarks):\n",
    "                mp_drawing.draw_landmarks(image, hand, mp_hands.HAND_CONNECTIONS,\n",
    "                                        mp_drawing.DrawingSpec(color=(0, 0, 0), thickness=2, circle_radius=4), # color = BGR\n",
    "                                        mp_drawing.DrawingSpec(color=(121, 44, 250), thickness=2, circle_radius=2))\n",
    "\n",
    "                # output hand left or right + accuracy score at coords calculated (wrist)\n",
    "                if get_label(num, hand, results): \n",
    "                    text, coord = get_label(num, hand, results)\n",
    "                    cv2.putText(image, text, coord, cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            \n",
    "                if (True): # need machine learning to train to detect the gesture we want \n",
    "                    cv2.putText(image, \"Status: DANGER\", (0, 25), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                    # root = tkinter.Tk()\n",
    "                    # root.withdraw()\n",
    "                    # messagebox.showinfo(\"Title\", \"Message\") # alert box once a hand is detected\n",
    "                    # break                \n",
    "        \n",
    "            # draw_finger_angles(image, results, joint_list)\n",
    "\n",
    "        cv2.imshow(\"Live Webcam Feed\", image) # render the image to the screen, and name it \"Hand Tracking\"\n",
    "\n",
    "        # Save images\n",
    "        # cv2.imwrite(os.path.join(\"Output Images\", \"{}.jpg\".format(uuid.uuid1())), image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord(\"q\"): # exit the webcam feed in 10ms by pressing q \n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Extract Landmarks and Angle Data & Export to CSV\n",
    "\n",
    "For our project, our goal is to detect a certain hand gesture among a crowd to signal an SOS.\n",
    "\n",
    "Thus, the most logical data to collect for this purpose is the hand landmarks. This will be our main variable.\n",
    "\n",
    "For more improvements, we could add in the angle of our joints.\n",
    "\n",
    "For further developement of such SOS gestures beyond our hands, we could add in face and pose landmarks, which are supported by MediaPipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of hand landmarks = 21\n",
    "# total number of x, y, z coordinates = 21 * 3 = 63\n",
    "variables = len(hand.landmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# form our column headers in csv file\n",
    "landmarks = ['class'] # class refers to the result, i.e., whether the hand shows the gesture or not\n",
    "for i in range(1, variables+1):\n",
    "    landmarks += [\"x{}\".format(i), \"y{}\".format(i), \"z{}\".format(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create csv file\n",
    "with open(\"var.csv\", mode = \"w\", newline = '') as f:\n",
    "    csv_writer = csv.writer(f, delimiter = \",\", quotechar = '\"', quoting = csv.QUOTE_MINIMAL)\n",
    "    csv_writer.writerow(landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name = \"SOS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0) # the input number 0 depends on your machine, it could be 1 or 2 as well\n",
    "\n",
    "# Detection Confidence: Threshold for initial detection to be successful\n",
    "# Tracking Confidence: Threshold for tracking after initial detection\n",
    "with mp_hands.Hands(min_detection_confidence = 0.5, min_tracking_confidence = 0.5, max_num_hands = 4) as hands:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read() # returns a return value and frame which is the image\n",
    "\n",
    "        # Get hand axis data from each frame; default 2 hands detectable\n",
    "        # In case your webcam is so old that it uses the color format BGR instead of RGB, uncomment the appropriate lines\n",
    "        image = cv2.flip(frame, 1) # flip on horizontal\n",
    "        # image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # webcam frame colour format is in BGR, but mediapipe takes in RGB\n",
    "        image.flags.writeable = False # lock the frame for us to process its data\n",
    "        results = hands.process(image) # get axis data\n",
    "        image.flags.writeable = True # unlock the frame for us to draw landmarks and connections\n",
    "        # image = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR) # convert mediapipe image back to BGR for webcam\n",
    "        \n",
    "        cv2.putText(image, \"Status: Safe\", (0, 25), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        if results.multi_hand_landmarks: # if we detect a hand, we draw the landmarks and the connections\n",
    "            for num, hand in enumerate(results.multi_hand_landmarks):\n",
    "                mp_drawing.draw_landmarks(image, hand, mp_hands.HAND_CONNECTIONS,\n",
    "                                        mp_drawing.DrawingSpec(color=(0, 0, 0), thickness=2, circle_radius=4), # color = BGR\n",
    "                                        mp_drawing.DrawingSpec(color=(121, 44, 250), thickness=2, circle_radius=2))\n",
    "\n",
    "                # output hand left or right + accuracy score at coords calculated (wrist)\n",
    "                if get_label(num, hand, results): \n",
    "                    text, coord = get_label(num, hand, results)\n",
    "                    cv2.putText(image, text, coord, cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            \n",
    "                if (True): # need machine learning to train to detect the gesture we want \n",
    "                    cv2.putText(image, \"Status: DANGER\", (0, 25), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                    # root = tkinter.Tk()\n",
    "                    # root.withdraw()\n",
    "                    # messagebox.showinfo(\"Title\", \"Message\") # alert box once a hand is detected\n",
    "                    # break                \n",
    "\n",
    "                # Export training data\n",
    "                # For better accuracy, take more samples\n",
    "                try:\n",
    "                    hand_landmark = hand.landmark\n",
    "                    hand_row = list(np.array([[landmark.x, landmark.y, landmark.z] for landmark in hand_landmark]).flatten())\n",
    "\n",
    "                    hand_row.insert(0, class_name)\n",
    "\n",
    "                    with open(\"var.csv\", mode = \"a\", newline = '') as f:\n",
    "                        csv_writer = csv.writer(f, delimiter = \",\", quotechar = '\"', quoting = csv.QUOTE_MINIMAL)\n",
    "                        csv_writer.writerow(hand_row)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            # draw_finger_angles(image, results, joint_list)\n",
    "\n",
    "\n",
    "        cv2.imshow(\"Live Webcam Feed\", image) # render the image to the screen, and name it \"Hand Tracking\"\n",
    "\n",
    "        # Save images\n",
    "        # cv2.imwrite(os.path.join(\"Output Images\", \"{}.jpg\".format(uuid.uuid1())), image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord(\"q\"): # exit the webcam feed in 10ms by pressing q \n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5858122706413269,\n",
       " 0.6939601302146912,\n",
       " 2.2907808272520924e-07,\n",
       " 0.527172327041626,\n",
       " 0.6743758916854858,\n",
       " -0.017742620781064034,\n",
       " 0.4777282774448395,\n",
       " 0.6202595233917236,\n",
       " -0.026190297678112984,\n",
       " 0.4417157471179962,\n",
       " 0.5779957175254822,\n",
       " -0.0341699980199337,\n",
       " 0.40599194169044495,\n",
       " 0.5589165091514587,\n",
       " -0.042631637305021286,\n",
       " 0.5123770833015442,\n",
       " 0.5076427459716797,\n",
       " -0.009192545898258686,\n",
       " 0.4823743402957916,\n",
       " 0.43902283906936646,\n",
       " -0.02386436052620411,\n",
       " 0.4639066755771637,\n",
       " 0.39624032378196716,\n",
       " -0.03798253834247589,\n",
       " 0.45025795698165894,\n",
       " 0.36031728982925415,\n",
       " -0.04888857528567314,\n",
       " 0.5487779378890991,\n",
       " 0.4913095533847809,\n",
       " -0.014613251201808453,\n",
       " 0.5304022431373596,\n",
       " 0.406402587890625,\n",
       " -0.026121854782104492,\n",
       " 0.520290195941925,\n",
       " 0.35374170541763306,\n",
       " -0.038568440824747086,\n",
       " 0.5126468539237976,\n",
       " 0.31152933835983276,\n",
       " -0.04809936508536339,\n",
       " 0.5848333835601807,\n",
       " 0.4945409893989563,\n",
       " -0.02359580807387829,\n",
       " 0.5789993405342102,\n",
       " 0.4121286869049072,\n",
       " -0.03821095451712608,\n",
       " 0.5750751495361328,\n",
       " 0.3622162938117981,\n",
       " -0.048640284687280655,\n",
       " 0.5699175000190735,\n",
       " 0.31862324476242065,\n",
       " -0.056147538125514984,\n",
       " 0.6205968260765076,\n",
       " 0.5127986073493958,\n",
       " -0.034396931529045105,\n",
       " 0.6302968859672546,\n",
       " 0.45092007517814636,\n",
       " -0.04720253869891167,\n",
       " 0.6355128884315491,\n",
       " 0.4113357961177826,\n",
       " -0.05165112391114235,\n",
       " 0.6382071375846863,\n",
       " 0.37431299686431885,\n",
       " -0.05489417165517807]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hand_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: CNN Models\n",
    "\n",
    "It is time to train our dataset of landmarks to detect our SOS gesture.  \n",
    "\n",
    "It seems like there are 2 popular open source solutions for neural network models other than building your own:\n",
    "Scikit-Learn \n",
    " \n",
    "    - Scikit-Learn\n",
    "    \n",
    "    - Tensorflow  \n",
    "    \n",
    "We shall be exploring and learning both of them since it is just a matter of applying rather than building an algorithm from scratch. At least, it is just a few hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U scikit-learn scipy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>z1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>z2</th>\n",
       "      <th>x3</th>\n",
       "      <th>y3</th>\n",
       "      <th>z3</th>\n",
       "      <th>...</th>\n",
       "      <th>z18</th>\n",
       "      <th>x19</th>\n",
       "      <th>y19</th>\n",
       "      <th>z19</th>\n",
       "      <th>x20</th>\n",
       "      <th>y20</th>\n",
       "      <th>z20</th>\n",
       "      <th>x21</th>\n",
       "      <th>y21</th>\n",
       "      <th>z21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SOS</td>\n",
       "      <td>0.659763</td>\n",
       "      <td>0.988639</td>\n",
       "      <td>4.275413e-07</td>\n",
       "      <td>0.592645</td>\n",
       "      <td>0.962583</td>\n",
       "      <td>-0.042312</td>\n",
       "      <td>0.542857</td>\n",
       "      <td>0.901450</td>\n",
       "      <td>-0.072470</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.070238</td>\n",
       "      <td>0.793026</td>\n",
       "      <td>0.736141</td>\n",
       "      <td>-0.094796</td>\n",
       "      <td>0.811189</td>\n",
       "      <td>0.684645</td>\n",
       "      <td>-0.105867</td>\n",
       "      <td>0.824053</td>\n",
       "      <td>0.636357</td>\n",
       "      <td>-0.112448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SOS</td>\n",
       "      <td>0.663425</td>\n",
       "      <td>0.931848</td>\n",
       "      <td>5.699486e-07</td>\n",
       "      <td>0.588070</td>\n",
       "      <td>0.861756</td>\n",
       "      <td>-0.026669</td>\n",
       "      <td>0.531753</td>\n",
       "      <td>0.772755</td>\n",
       "      <td>-0.041814</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036531</td>\n",
       "      <td>0.778460</td>\n",
       "      <td>0.604706</td>\n",
       "      <td>-0.050955</td>\n",
       "      <td>0.796390</td>\n",
       "      <td>0.552505</td>\n",
       "      <td>-0.057252</td>\n",
       "      <td>0.810432</td>\n",
       "      <td>0.500476</td>\n",
       "      <td>-0.061279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SOS</td>\n",
       "      <td>0.655319</td>\n",
       "      <td>0.862647</td>\n",
       "      <td>5.972269e-07</td>\n",
       "      <td>0.575160</td>\n",
       "      <td>0.805765</td>\n",
       "      <td>-0.030462</td>\n",
       "      <td>0.520258</td>\n",
       "      <td>0.720630</td>\n",
       "      <td>-0.047605</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.047771</td>\n",
       "      <td>0.764324</td>\n",
       "      <td>0.544507</td>\n",
       "      <td>-0.069305</td>\n",
       "      <td>0.781560</td>\n",
       "      <td>0.491967</td>\n",
       "      <td>-0.079103</td>\n",
       "      <td>0.793906</td>\n",
       "      <td>0.439828</td>\n",
       "      <td>-0.085596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SOS</td>\n",
       "      <td>0.642661</td>\n",
       "      <td>0.810859</td>\n",
       "      <td>5.846537e-07</td>\n",
       "      <td>0.562953</td>\n",
       "      <td>0.772985</td>\n",
       "      <td>-0.034952</td>\n",
       "      <td>0.505502</td>\n",
       "      <td>0.686717</td>\n",
       "      <td>-0.052515</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.048999</td>\n",
       "      <td>0.755159</td>\n",
       "      <td>0.508332</td>\n",
       "      <td>-0.073870</td>\n",
       "      <td>0.773151</td>\n",
       "      <td>0.455497</td>\n",
       "      <td>-0.086857</td>\n",
       "      <td>0.784920</td>\n",
       "      <td>0.403252</td>\n",
       "      <td>-0.095704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SOS</td>\n",
       "      <td>0.627298</td>\n",
       "      <td>0.801663</td>\n",
       "      <td>5.177911e-07</td>\n",
       "      <td>0.546639</td>\n",
       "      <td>0.761923</td>\n",
       "      <td>-0.034101</td>\n",
       "      <td>0.486315</td>\n",
       "      <td>0.679540</td>\n",
       "      <td>-0.051714</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.056262</td>\n",
       "      <td>0.729574</td>\n",
       "      <td>0.496518</td>\n",
       "      <td>-0.081211</td>\n",
       "      <td>0.749899</td>\n",
       "      <td>0.444681</td>\n",
       "      <td>-0.092765</td>\n",
       "      <td>0.765560</td>\n",
       "      <td>0.394522</td>\n",
       "      <td>-0.100449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  class        x1        y1            z1        x2        y2        z2  \\\n",
       "0   SOS  0.659763  0.988639  4.275413e-07  0.592645  0.962583 -0.042312   \n",
       "1   SOS  0.663425  0.931848  5.699486e-07  0.588070  0.861756 -0.026669   \n",
       "2   SOS  0.655319  0.862647  5.972269e-07  0.575160  0.805765 -0.030462   \n",
       "3   SOS  0.642661  0.810859  5.846537e-07  0.562953  0.772985 -0.034952   \n",
       "4   SOS  0.627298  0.801663  5.177911e-07  0.546639  0.761923 -0.034101   \n",
       "\n",
       "         x3        y3        z3  ...       z18       x19       y19       z19  \\\n",
       "0  0.542857  0.901450 -0.072470  ... -0.070238  0.793026  0.736141 -0.094796   \n",
       "1  0.531753  0.772755 -0.041814  ... -0.036531  0.778460  0.604706 -0.050955   \n",
       "2  0.520258  0.720630 -0.047605  ... -0.047771  0.764324  0.544507 -0.069305   \n",
       "3  0.505502  0.686717 -0.052515  ... -0.048999  0.755159  0.508332 -0.073870   \n",
       "4  0.486315  0.679540 -0.051714  ... -0.056262  0.729574  0.496518 -0.081211   \n",
       "\n",
       "        x20       y20       z20       x21       y21       z21  \n",
       "0  0.811189  0.684645 -0.105867  0.824053  0.636357 -0.112448  \n",
       "1  0.796390  0.552505 -0.057252  0.810432  0.500476 -0.061279  \n",
       "2  0.781560  0.491967 -0.079103  0.793906  0.439828 -0.085596  \n",
       "3  0.773151  0.455497 -0.086857  0.784920  0.403252 -0.095704  \n",
       "4  0.749899  0.444681 -0.092765  0.765560  0.394522 -0.100449  \n",
       "\n",
       "[5 rows x 64 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read dataset\n",
    "df = pd.read_csv(\"var.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train and set\n",
    "x = df.drop(\"class\", axis = 1)\n",
    "y = df[\"class\"]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(alpha=1e-05, hidden_layer_sizes=(63, 31, 16, 8, 4),\n",
       "              random_state=1, solver=&#x27;lbfgs&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(alpha=1e-05, hidden_layer_sizes=(63, 31, 16, 8, 4),\n",
       "              random_state=1, solver=&#x27;lbfgs&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(alpha=1e-05, hidden_layer_sizes=(63, 31, 16, 8, 4),\n",
       "              random_state=1, solver='lbfgs')"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We are using MLPClassifer from Scikit-learn for this implementation\n",
    "# There are a few parameters in MLPClassifier that we have to hypertune, which is one of the disadvantages of this neural network model,\n",
    "# but this is unavoided because we do not have the computing power for a more advanced one\n",
    "# Here are the more important parameters:\n",
    "    # solver: {'lbfgs', 'sgd', 'adam'}; adam works well for large datasets (>10000), lbfgs works better for smaller datasets\n",
    "    # alpha: strength of L2 regularization\n",
    "    # hidden_layer_sizes: number of neurons in the ith hidden layer; default = (100,)\n",
    "clf = MLPClassifier(solver = 'lbfgs', alpha = 1e-5, hidden_layer_sizes = (63, 31, 16, 8, 4,), random_state = 1)\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of MLPClassifier = 100.0%\n"
     ]
    }
   ],
   "source": [
    "yhat = clf.predict(x_test)\n",
    "print(\"Accuracy of MLPClassifier = \" + str(accuracy_score(y_test, yhat) * 100.0) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save our model as pickle file\n",
    "# pickle file is recommended in the data science field, but you can export it as a python file too for other editors\n",
    "with open(\"sos-gesture.pkl\", \"wb\") as f:\n",
    "    pickle.dump(clf, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of MLP\n",
    "- Capability to learn non-linear models.\n",
    "- Capability to learn models in real-time (on-line learning)\n",
    "\n",
    "### Disadvantages of MLP\n",
    "- MLP with hidden layers have a non-convex loss function where there exists more than one local minimum. Therefore different random weight initializations can lead to different validation accuracy.\n",
    "- MLP requires tuning a number of hyperparameters such as the number of hidden neurons, layers, and iterations.\n",
    "- MLP is sensitive to feature scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Make Detections with our Models in Real-time\n",
    "\n",
    "Instead of appending our live hand gesture data into our csv, we will load that as rows into our model to predict whether it is the gesture we want or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sos-gesture.pkl\", \"rb\") as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(alpha=1e-05, hidden_layer_sizes=(63, 31, 16, 8, 4),\n",
       "              random_state=1, solver=&#x27;lbfgs&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(alpha=1e-05, hidden_layer_sizes=(63, 31, 16, 8, 4),\n",
       "              random_state=1, solver=&#x27;lbfgs&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(alpha=1e-05, hidden_layer_sizes=(63, 31, 16, 8, 4),\n",
       "              random_state=1, solver='lbfgs')"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0) # the input number 0 depends on your machine, it could be 1 or 2 as well\n",
    "\n",
    "# Detection Confidence: Threshold for initial detection to be successful\n",
    "# Tracking Confidence: Threshold for tracking after initial detection\n",
    "with mp_hands.Hands(min_detection_confidence = 0.5, min_tracking_confidence = 0.5, max_num_hands = 4) as hands:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read() # returns a return value and frame which is the image\n",
    "\n",
    "        # Get hand axis data from each frame; default 2 hands detectable\n",
    "        # In case your webcam is so old that it uses the color format BGR instead of RGB, uncomment the appropriate lines\n",
    "        image = cv2.flip(frame, 1) # flip on horizontal\n",
    "        # image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # webcam frame colour format is in BGR, but mediapipe takes in RGB\n",
    "        image.flags.writeable = False # lock the frame for us to process its data\n",
    "        results = hands.process(image) # get axis data\n",
    "        image.flags.writeable = True # unlock the frame for us to draw landmarks and connections\n",
    "        # image = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR) # convert mediapipe image back to BGR for webcam\n",
    "        \n",
    "        cv2.putText(image, \"Status: Safe\", (0, 25), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        if results.multi_hand_landmarks: # if we detect a hand, we draw the landmarks and the connections\n",
    "            for num, hand in enumerate(results.multi_hand_landmarks):\n",
    "                mp_drawing.draw_landmarks(image, hand, mp_hands.HAND_CONNECTIONS,\n",
    "                                        mp_drawing.DrawingSpec(color=(0, 0, 0), thickness=2, circle_radius=4), # color = BGR\n",
    "                                        mp_drawing.DrawingSpec(color=(121, 44, 250), thickness=2, circle_radius=2))\n",
    "\n",
    "                # output hand left or right + accuracy score at coords calculated (wrist)\n",
    "                if get_label(num, hand, results): \n",
    "                    text, coord = get_label(num, hand, results)\n",
    "                    cv2.putText(image, text, coord, cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            \n",
    "                if (True): # need machine learning to train to detect the gesture we want \n",
    "                    cv2.putText(image, \"Status: DANGER\", (0, 25), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                    # root = tkinter.Tk()\n",
    "                    # root.withdraw()\n",
    "                    # messagebox.showinfo(\"Title\", \"Message\") # alert box once a hand is detected\n",
    "                    # break                \n",
    "\n",
    "                # Export training data\n",
    "                # For better accuracy, take more samples\n",
    "                try:\n",
    "                    hand_landmark = hand.landmark\n",
    "                    hand_row = list(np.array([[landmark.x, landmark.y, landmark.z] for landmark in hand_landmark]).flatten())\n",
    "                    \n",
    "                    # Pass in our live data into our model\n",
    "                    X = pd.DataFrame([hand_row])\n",
    "                    sos_gesture_class = model.predict(X)[0] # classify whether it is our gesture or not\n",
    "                    sos_gesture_prob = model.predict_proba(X)[0] # probability of the class; this will be an array of multiple values\n",
    "                    \n",
    "                    # Print the result onto the screen\n",
    "                    \n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            # draw_finger_angles(image, results, joint_list)\n",
    "\n",
    "\n",
    "        cv2.imshow(\"Live Webcam Feed\", image) # render the image to the screen, and name it \"Hand Tracking\"\n",
    "\n",
    "        # Save images\n",
    "        # cv2.imwrite(os.path.join(\"Output Images\", \"{}.jpg\".format(uuid.uuid1())), image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord(\"q\"): # exit the webcam feed in 10ms by pressing q \n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38c80501d2b2e4a9d6df779588d376b0530846b06a79a3e25422ca44224fd643"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
